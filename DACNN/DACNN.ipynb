{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T05:19:01.470746Z",
     "start_time": "2025-11-09T05:19:01.343269Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:19:02.592320Z",
     "start_time": "2025-11-09T05:19:02.541942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DACNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DACNN, self).__init__()\n",
    "\n",
    "        # 根据表格构建网络结构\n",
    "        # 第1层: Conv1 + MaxPool\n",
    "        self.shared_conv1 = nn.Conv1d(1, 8, kernel_size=32, stride=2)  # 输出: 1009×8\n",
    "        self.shared_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 504×8\n",
    "\n",
    "        # 第2层: Conv2 + MaxPool\n",
    "        self.shared_conv2 = nn.Conv1d(8, 16, kernel_size=16, stride=2)  # 输出: 245×16\n",
    "        self.shared_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 122×16\n",
    "\n",
    "        # 第3层: Conv3 + MaxPool\n",
    "        self.shared_conv3 = nn.Conv1d(16, 32, kernel_size=8, stride=2)  # 输出: 58×32\n",
    "        self.shared_pool3 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 29×32\n",
    "\n",
    "        # 第4层: 源域和目标域特定的卷积层\n",
    "        self.source_conv4 = nn.Conv1d(32, 32, kernel_size=8, stride=2)  # 输出: 29×32\n",
    "        self.source_pool4 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 14×32\n",
    "\n",
    "        self.target_conv4 = nn.Conv1d(32, 32, kernel_size=8, stride=2)  # 输出: 29×32\n",
    "        self.target_pool4 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 14×32\n",
    "\n",
    "        # 第5层: 源域和目标域特定的卷积层\n",
    "        self.source_conv5 = nn.Conv1d(32, 64, kernel_size=3, stride=2)  # 输出: 2×64\n",
    "        self.source_pool5 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 1×64\n",
    "\n",
    "        self.target_conv5 = nn.Conv1d(32, 64, kernel_size=3, stride=2)  # 输出: 2×64\n",
    "        self.target_pool5 = nn.MaxPool1d(kernel_size=2, stride=2)  # 输出: 1×64\n",
    "\n",
    "        # 全连接层\n",
    "        self.shared_fc1 = nn.Linear(64, 500)  # 输入: 1×64 = 64\n",
    "        self.source_fc2 = nn.Linear(500, num_classes)\n",
    "        self.target_fc2 = nn.Linear(500, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def source_forward(self, x):\n",
    "        # 共享层\n",
    "        x = self.relu(self.shared_conv1(x))\n",
    "        x = self.shared_pool1(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv2(x))\n",
    "        x = self.shared_pool2(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv3(x))\n",
    "        x = self.shared_pool3(x)\n",
    "\n",
    "        # 源域特定层\n",
    "        x = self.relu(self.source_conv4(x))\n",
    "        x = self.source_pool4(x)\n",
    "\n",
    "        x = self.relu(self.source_conv5(x))\n",
    "        x = self.source_pool5(x)\n",
    "\n",
    "        # 全连接层\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.shared_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.source_fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def target_forward(self, x):\n",
    "        # 共享层\n",
    "        x = self.relu(self.shared_conv1(x))\n",
    "        x = self.shared_pool1(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv2(x))\n",
    "        x = self.shared_pool2(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv3(x))\n",
    "        x = self.shared_pool3(x)\n",
    "\n",
    "        # 目标域特定层\n",
    "        x = self.relu(self.target_conv4(x))\n",
    "        x = self.target_pool4(x)\n",
    "\n",
    "        x = self.relu(self.target_conv5(x))\n",
    "        x = self.target_pool5(x)\n",
    "\n",
    "        # 全连接层\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.shared_fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.target_fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def source_features(self, x):\n",
    "        # 获取源域特征（用于MMD计算）\n",
    "        x = self.relu(self.shared_conv1(x))\n",
    "        x = self.shared_pool1(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv2(x))\n",
    "        x = self.shared_pool2(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv3(x))\n",
    "        x = self.shared_pool3(x)\n",
    "\n",
    "        x = self.relu(self.source_conv4(x))\n",
    "        x = self.source_pool4(x)\n",
    "\n",
    "        x = self.relu(self.source_conv5(x))\n",
    "        x = self.source_pool5(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.relu(self.shared_fc1(x))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def target_features(self, x):\n",
    "        # 获取目标域特征（用于MMD计算）\n",
    "        x = self.relu(self.shared_conv1(x))\n",
    "        x = self.shared_pool1(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv2(x))\n",
    "        x = self.shared_pool2(x)\n",
    "\n",
    "        x = self.relu(self.shared_conv3(x))\n",
    "        x = self.shared_pool3(x)\n",
    "\n",
    "        x = self.relu(self.target_conv4(x))\n",
    "        x = self.target_pool4(x)\n",
    "\n",
    "        x = self.relu(self.target_conv5(x))\n",
    "        x = self.target_pool5(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.relu(self.shared_fc1(x))\n",
    "\n",
    "        return features"
   ],
   "id": "c80b01028d4e4bb2",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:19:06.184342Z",
     "start_time": "2025-11-09T05:19:06.142227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_mul=2.0, kernel_num=5):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples ** 2 - n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul ** i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.guassian_kernel(source, target, kernel_mul=self.kernel_mul,\n",
    "                                     kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX + YY - XY - YX)\n",
    "        return loss"
   ],
   "id": "594999f557864044",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:19:08.142790Z",
     "start_time": "2025-11-09T05:19:08.134048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CWRUDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ],
   "id": "8e611bb04985942",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:36:25.232145Z",
     "start_time": "2025-11-09T05:36:25.197503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_from_structure(data_path, load_conditions=['1', '2', '3']):\n",
    "    \"\"\"从您提供的目录结构加载数据\"\"\"\n",
    "\n",
    "    # 类别映射：10个类别（9种故障 + 1种正常）\n",
    "    # 类别0: 正常 (N)\n",
    "    # 类别1: IF_0.007\n",
    "    # 类别2: IF_0.014\n",
    "    # 类别3: IF_0.021\n",
    "    # 类别4: BF_0.007\n",
    "    # 类别5: BF_0.014\n",
    "    # 类别6: BF_0.021\n",
    "    # 类别7: OF_0.007\n",
    "    # 类别8: OF_0.014\n",
    "    # 类别9: OF_0.021\n",
    "\n",
    "    domains = {}\n",
    "\n",
    "    for load_condition in load_conditions:\n",
    "        domain_data = []\n",
    "        domain_labels = []\n",
    "        load_path = os.path.join(data_path, load_condition)\n",
    "\n",
    "        if not os.path.exists(load_path):\n",
    "            print(f\"Warning: Load condition {load_condition} path {load_path} does not exist\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing load condition {load_condition}...\")\n",
    "\n",
    "        # 类别0: 正常状态\n",
    "        normal_file = os.path.join(load_path, 'N.mat')\n",
    "        if os.path.exists(normal_file):\n",
    "            print(f\"  Processing normal condition...\")\n",
    "            samples = process_mat_file(normal_file, 800, 0)\n",
    "            domain_data.extend(samples)\n",
    "            domain_labels.extend([0] * len(samples))\n",
    "\n",
    "        # 故障类型和对应的尺寸\n",
    "        fault_types = ['IF', 'BF', 'OF']\n",
    "        fault_sizes = ['7', '14', '21']\n",
    "\n",
    "        label_counter = 1\n",
    "        for fault_type in fault_types:\n",
    "            for fault_size in fault_sizes:\n",
    "                fault_dir = os.path.join(load_path, fault_size)\n",
    "                if not os.path.exists(fault_dir):\n",
    "                    continue\n",
    "\n",
    "                # 查找对应的故障文件\n",
    "                fault_pattern = os.path.join(fault_dir, f'{fault_type}*.mat')\n",
    "                fault_files = glob.glob(fault_pattern)\n",
    "\n",
    "                if fault_files:\n",
    "                    # 取第一个匹配的文件\n",
    "                    fault_file = fault_files[0]\n",
    "                    print(f\"  Processing {fault_type}_{fault_size}...\")\n",
    "                    samples = process_mat_file(fault_file, 800, label_counter)\n",
    "                    domain_data.extend(samples)\n",
    "                    domain_labels.extend([label_counter] * len(samples))\n",
    "\n",
    "                label_counter += 1\n",
    "\n",
    "        if domain_data:\n",
    "            domains[load_condition] = {\n",
    "                'data': np.array(domain_data, dtype=np.float32),\n",
    "                'labels': np.array(domain_labels, dtype=np.int64)\n",
    "            }\n",
    "            print(f\"Domain {load_condition}: {len(domain_data)} samples\")\n",
    "        else:\n",
    "            print(f\"Warning: No data found for domain {load_condition}\")\n",
    "\n",
    "    return domains\n",
    "\n",
    "def process_mat_file(file_path, num_samples, label):\n",
    "    \"\"\"处理单个.mat文件，生成指定数量的样本\"\"\"\n",
    "    try:\n",
    "        # 加载MAT文件\n",
    "        mat_data = sio.loadmat(file_path)\n",
    "\n",
    "        # 找到包含振动数据的变量（排除元数据变量）\n",
    "        data_key = None\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__') and not key.startswith('header') and not key.startswith('version'):\n",
    "                # 检查是否是数值数组\n",
    "                data = mat_data[key]\n",
    "                if isinstance(data, np.ndarray) and data.size > 1000:\n",
    "                    data_key = key\n",
    "                    break\n",
    "\n",
    "        if data_key is None:\n",
    "            print(f\"Warning: No suitable data found in {file_path}\")\n",
    "            # 生成合成数据用于测试\n",
    "            vibration_data = np.random.randn(100000)\n",
    "        else:\n",
    "            vibration_data = mat_data[data_key].flatten()\n",
    "\n",
    "        samples = []\n",
    "        segment_length = 4096\n",
    "\n",
    "        # 确保有足够的数据\n",
    "        if len(vibration_data) < segment_length:\n",
    "            print(f\"Warning: Data in {file_path} is too short ({len(vibration_data)} < {segment_length})\")\n",
    "            # 重复数据以达到所需长度\n",
    "            repeats = (segment_length // len(vibration_data)) + 1\n",
    "            vibration_data = np.tile(vibration_data, repeats)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # 随机选择起始位置\n",
    "            start_idx = np.random.randint(0, len(vibration_data) - segment_length)\n",
    "            segment = vibration_data[start_idx:start_idx + segment_length]\n",
    "\n",
    "            # FFT变换\n",
    "            fft_segment = np.fft.fft(segment)\n",
    "            fft_magnitude = np.abs(fft_segment)[:2048]  # 取前2048个系数\n",
    "\n",
    "            # 归一化\n",
    "            fft_magnitude = (fft_magnitude - np.mean(fft_magnitude)) / (np.std(fft_magnitude) + 1e-8)\n",
    "\n",
    "            samples.append(fft_magnitude)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        # 生成合成数据\n",
    "        print(\"Generating synthetic data for testing...\")\n",
    "        samples = []\n",
    "        for i in range(num_samples):\n",
    "            synthetic_fft = np.random.randn(2048)\n",
    "            samples.append(synthetic_fft)\n",
    "        return samples\n"
   ],
   "id": "5ae32b17aea985df",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:36:38.976489Z",
     "start_time": "2025-11-09T05:36:38.956216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pretrain_model(model, source_dataloader, device, num_epochs=100):\n",
    "    \"\"\"预训练阶段\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_data, batch_labels in source_dataloader:\n",
    "            batch_data = batch_data.unsqueeze(1).to(device)  # 添加channel维度\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.source_forward(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Pre-train Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(source_dataloader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "def domain_adaptive_finetune(model, source_dataloader, target_dataloader, device, num_epochs=100, lambda_mmd=1.0):\n",
    "    \"\"\"域自适应微调阶段\"\"\"\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_mmd = MMDLoss()\n",
    "\n",
    "    # 只优化目标域特定的参数\n",
    "    target_params = list(model.target_conv4.parameters()) + list(model.target_conv5.parameters()) + \\\n",
    "                   list(model.target_fc2.parameters())\n",
    "    optimizer = optim.Adam(target_params, lr=0.0001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_mmd_loss = 0\n",
    "\n",
    "        source_iter = iter(source_dataloader)\n",
    "        target_iter = iter(target_dataloader)\n",
    "\n",
    "        num_batches = min(len(source_dataloader), len(target_dataloader))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            try:\n",
    "                # 获取源域和目标域批次数据\n",
    "                source_data, source_labels = next(source_iter)\n",
    "                target_data, _ = next(target_iter)\n",
    "\n",
    "                source_data = source_data.unsqueeze(1).to(device)\n",
    "                source_labels = source_labels.to(device)\n",
    "                target_data = target_data.unsqueeze(1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 源域分类损失\n",
    "                source_outputs = model.source_forward(source_data)\n",
    "                cls_loss = criterion_cls(source_outputs, source_labels)\n",
    "\n",
    "                # MMD损失\n",
    "                source_features = model.source_features(source_data)\n",
    "                target_features = model.target_features(target_data)\n",
    "                mmd_loss = criterion_mmd(source_features, target_features)\n",
    "\n",
    "                # 总损失\n",
    "                loss = cls_loss + lambda_mmd * mmd_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_cls_loss += cls_loss.item()\n",
    "                total_mmd_loss += mmd_loss.item()\n",
    "\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            avg_cls_loss = total_cls_loss / num_batches\n",
    "            avg_mmd_loss = total_mmd_loss / num_batches\n",
    "            print(f'Fine-tune Epoch [{epoch+1}/{num_epochs}], Total Loss: {avg_loss:.4f}, '\n",
    "                  f'Cls Loss: {avg_cls_loss:.4f}, MMD Loss: {avg_mmd_loss:.4f}')\n",
    "\n",
    "    return model"
   ],
   "id": "48f624c6bca63702",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:19:15.443184Z",
     "start_time": "2025-11-09T05:19:15.430922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataloader, device, domain='target'):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            batch_data = batch_data.unsqueeze(1).to(device)\n",
    "\n",
    "            if domain == 'target':\n",
    "                outputs = model.target_forward(batch_data)\n",
    "            else:\n",
    "                outputs = model.source_forward(batch_data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, all_predictions, all_labels"
   ],
   "id": "e3d182a92971c9a9",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T05:36:54.837892Z",
     "start_time": "2025-11-09T05:36:43.720955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # 参数设置\n",
    "    data_path = r\"D:\\deskbook\\科研\\数据集\\cwru\\data\"  # 修改为实际数据路径\n",
    "    batch_size = 64\n",
    "    num_epochs_pretrain = 100\n",
    "    num_epochs_finetune = 100\n",
    "    lambda_mmd = 1.0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 加载数据（只使用负载1、2、3）\n",
    "    print(\"Loading data from directory structure...\")\n",
    "    domains = load_data_from_structure(data_path, load_conditions=['1', '2', '3'])\n",
    "\n",
    "    # 定义域适配任务（例如 1->2）\n",
    "    source_domain = '2'\n",
    "    target_domain = '3'\n",
    "\n",
    "    if source_domain not in domains or target_domain not in domains:\n",
    "        print(f\"Error: Required domains not found. Available domains: {list(domains.keys())}\")\n",
    "        return\n",
    "\n",
    "    source_data = domains[source_domain]['data']\n",
    "    source_labels = domains[source_domain]['labels']\n",
    "    target_data = domains[target_domain]['data']\n",
    "    target_labels = domains[target_domain]['labels']\n",
    "\n",
    "    print(f\"Source domain {source_domain}: {len(source_data)} samples\")\n",
    "    print(f\"Target domain {target_domain}: {len(target_data)} samples\")\n",
    "\n",
    "    # 创建数据加载器\n",
    "    source_dataset = CWRUDataset(source_data, source_labels)\n",
    "    target_dataset = CWRUDataset(target_data, target_labels)\n",
    "\n",
    "    source_dataloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_dataloader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 创建模型\n",
    "    model = DACNN(num_classes=10).to(device)\n",
    "\n",
    "    # 阶段1: 预训练\n",
    "    print(\"Stage 1: Pre-training on source domain...\")\n",
    "    model = pretrain_model(model, source_dataloader, device, num_epochs_pretrain)\n",
    "\n",
    "    # 评估预训练模型在源域和目标域的性能\n",
    "    print(\"\\nEvaluating pre-trained model...\")\n",
    "    source_accuracy, source_precision, source_recall, _, _ = evaluate_model(\n",
    "        model, DataLoader(source_dataset, batch_size=batch_size, shuffle=False), device, 'source')\n",
    "\n",
    "    target_accuracy, target_precision, target_recall, _, _ = evaluate_model(\n",
    "        model, DataLoader(target_dataset, batch_size=batch_size, shuffle=False), device, 'source')\n",
    "\n",
    "    print(f\"Pre-trained model - Source Domain: Accuracy: {source_accuracy:.4f}, \"\n",
    "          f\"Precision: {source_precision:.4f}, Recall: {source_recall:.4f}\")\n",
    "    print(f\"Pre-trained model - Target Domain: Accuracy: {target_accuracy:.4f}, \"\n",
    "          f\"Precision: {target_precision:.4f}, Recall: {target_recall:.4f}\")\n",
    "\n",
    "    # 阶段2: 域自适应微调\n",
    "    print(\"\\nStage 2: Domain adaptive fine-tuning...\")\n",
    "    model = domain_adaptive_finetune(model, source_dataloader, target_dataloader,\n",
    "                                   device, num_epochs_finetune, lambda_mmd)\n",
    "\n",
    "    # 评估最终模型\n",
    "    print(\"\\nEvaluating fine-tuned model...\")\n",
    "    final_accuracy, final_precision, final_recall, predictions, labels = evaluate_model(\n",
    "        model, DataLoader(target_dataset, batch_size=batch_size, shuffle=False), device, 'target')\n",
    "\n",
    "    print(f\"Fine-tuned model - Target Domain: Accuracy: {final_accuracy:.4f}, \"\n",
    "          f\"Precision: {final_precision:.4f}, Recall: {final_recall:.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), f'dacnn_model_{source_domain}_to_{target_domain}.pth')\n",
    "    print(f\"Model saved as dacnn_model_{source_domain}_to_{target_domain}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "28786116d74cae97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data from directory structure...\n",
      "Processing load condition 1...\n",
      "  Processing normal condition...\n",
      "  Processing IF_7...\n",
      "  Processing IF_14...\n",
      "  Processing IF_21...\n",
      "  Processing BF_7...\n",
      "  Processing BF_14...\n",
      "  Processing BF_21...\n",
      "  Processing OF_7...\n",
      "  Processing OF_14...\n",
      "  Processing OF_21...\n",
      "Domain 1: 8000 samples\n",
      "Processing load condition 2...\n",
      "  Processing normal condition...\n",
      "  Processing IF_7...\n",
      "  Processing IF_14...\n",
      "  Processing IF_21...\n",
      "  Processing BF_7...\n",
      "  Processing BF_14...\n",
      "  Processing BF_21...\n",
      "  Processing OF_7...\n",
      "  Processing OF_14...\n",
      "  Processing OF_21...\n",
      "Domain 2: 8000 samples\n",
      "Processing load condition 3...\n",
      "  Processing normal condition...\n",
      "  Processing IF_7...\n",
      "  Processing IF_14...\n",
      "  Processing IF_21...\n",
      "  Processing BF_7...\n",
      "  Processing BF_14...\n",
      "  Processing BF_21...\n",
      "  Processing OF_7...\n",
      "  Processing OF_14...\n",
      "  Processing OF_21...\n",
      "Domain 3: 8000 samples\n",
      "Source domain 2: 8000 samples\n",
      "Target domain 3: 8000 samples\n",
      "Stage 1: Pre-training on source domain...\n",
      "Pre-train Epoch [2/100], Loss: 0.0003, Accuracy: 100.00%\n",
      "Pre-train Epoch [4/100], Loss: 0.0001, Accuracy: 100.00%\n",
      "Pre-train Epoch [6/100], Loss: 0.0000, Accuracy: 100.00%\n",
      "Pre-train Epoch [8/100], Loss: 0.0008, Accuracy: 99.99%\n",
      "Pre-train Epoch [10/100], Loss: 0.0000, Accuracy: 100.00%\n",
      "Pre-train Epoch [12/100], Loss: 0.0000, Accuracy: 100.00%\n",
      "Pre-train Epoch [14/100], Loss: 0.0000, Accuracy: 100.00%\n",
      "Pre-train Epoch [16/100], Loss: 0.0000, Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 77\u001B[39m\n\u001B[32m     74\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel saved as dacnn_model_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msource_domain\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_to_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_domain\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.pth\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 44\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# 阶段1: 预训练\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStage 1: Pre-training on source domain...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m model = \u001B[43mpretrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs_pretrain\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m# 评估预训练模型在源域和目标域的性能\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mEvaluating pre-trained model...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mpretrain_model\u001B[39m\u001B[34m(model, source_dataloader, device, num_epochs)\u001B[39m\n\u001B[32m     17\u001B[39m outputs = model.source_forward(batch_data)\n\u001B[32m     18\u001B[39m loss = criterion(outputs, batch_labels)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m optimizer.step()\n\u001B[32m     22\u001B[39m total_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda3\\envs\\EWSnet\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    571\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    572\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    573\u001B[39m         Tensor.backward,\n\u001B[32m    574\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    579\u001B[39m         inputs=inputs,\n\u001B[32m    580\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m581\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    582\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    583\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda3\\envs\\EWSnet\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\anaconda3\\envs\\EWSnet\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    823\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    824\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    826\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    827\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    829\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# ===== 改进后的 DACNN 模型 =====\n",
    "class DACNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DACNN, self).__init__()\n",
    "\n",
    "        # 共享卷积部分\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=32, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "\n",
    "            nn.Conv1d(8, 16, kernel_size=16, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=8, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 源域和目标域特有卷积部分\n",
    "        self.source_branch = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, kernel_size=8, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.target_branch = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, kernel_size=8, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 全连接部分\n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Linear(64, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc_source = nn.Linear(500, num_classes)\n",
    "        self.fc_target = nn.Linear(500, num_classes)\n",
    "\n",
    "    # ===== 源域前向 =====\n",
    "    def source_forward(self, x):\n",
    "        x = self.shared_conv(x)\n",
    "        x = self.source_branch(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_shared(x)\n",
    "        out = self.fc_source(x)\n",
    "        return out\n",
    "\n",
    "    # ===== 目标域前向 =====\n",
    "    def target_forward(self, x):\n",
    "        x = self.shared_conv(x)\n",
    "        x = self.target_branch(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_shared(x)\n",
    "        out = self.fc_target(x)\n",
    "        return out\n",
    "\n",
    "    # ===== 提取特征用于 MMD =====\n",
    "    def extract_features(self, x, domain='source'):\n",
    "        x = self.shared_conv(x)\n",
    "        x = self.source_branch(x) if domain == 'source' else self.target_branch(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.fc_shared[0:2](x)  # 仅取到 ReLU，不包括 Dropout\n",
    "        return features\n",
    "\n",
    "    # ===== 用于 summary 的统一 forward（默认 source）=====\n",
    "    def forward(self, x, domain='source'):\n",
    "        if domain == 'target':\n",
    "            return self.target_forward(x)\n",
    "        return self.source_forward(x)\n",
    "\n",
    "\n",
    "# 测试模型结构\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DACNN(num_classes=10).to(device)\n",
    "summary(model, input_size=(1, 2048))\n"
   ],
   "id": "cabcfb448ae4acf1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
